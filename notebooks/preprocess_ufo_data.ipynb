{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UFO Data Preprocessing Notebook üöÄüëΩüè∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[!WARNING]  \n",
    "Download the data before running this notebook. Check the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading üì•\n",
    "\n",
    "In this cell, we load the UFO raw dataset:\n",
    "\n",
    "- **UFO Data:** Loaded from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# SETUP: Define output directory relative to this script\n",
    "# ============================================================\n",
    "# Get the absolute path of the directory where this script is located\n",
    "# In a notebook, __file__ is not defined so we use os.getcwd() as a fallback.\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()\n",
    "\n",
    "# Define the folder where raw data is stored (assumed to be \"../data/raw\")\n",
    "RAW_DIR = os.path.join(BASE_DIR, \"..\", \"data\", \"raw\")\n",
    "\n",
    "# Define the folder where processed data will be saved (assumed to be \"../data/processed\")\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, \"..\", \"data\", \"processed\")\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Cell 1: Data Loading üì•\n",
    "# -------------------------------------------------------------------\n",
    "# Build the absolute paths for each dataset\n",
    "ufo_path = os.path.join(RAW_DIR, \"nuforc_reports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UFO data from CSV using the absolute path\n",
    "ufo_df = pd.read_csv(ufo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering UFO Data for USA Reports üá∫üá∏\n",
    "\n",
    "This cell filters the UFO dataset so that we only work with USA reports and removes rows missing both the city and state information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter UFO data to include only reports from the USA\n",
    "ufo_df = ufo_df.loc[ufo_df['country'] == \"USA\"]\n",
    "\n",
    "# Drop rows that have missing values in both 'city' and 'state'\n",
    "ufo_df.dropna(subset=['city', 'state'], how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocoding UFO Report Locations üåç\n",
    "\n",
    "We use the geopy library to obtain geographic coordinates (latitude and longitude) for each UFO report based on its city, state, and country.\n",
    "\n",
    "- **Nominatim** is used as the geocoder.\n",
    "- **RateLimiter** is applied to avoid hitting the request limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "# Initialize the Nominatim geocoder with a custom user agent\n",
    "geolocator = Nominatim(user_agent=\"ufo_project\")\n",
    "# Use RateLimiter to ensure at least one second delay between geocoding calls\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "\n",
    "def get_coordinates(row):\n",
    "    # Construct the location string from city, state, and country\n",
    "    location = geocode(f\"{row['city']}, {row['state']}, {row['country']}\")\n",
    "    print(location)  # Debug: print the location result\n",
    "    if location:\n",
    "        return location.latitude, location.longitude\n",
    "    else:\n",
    "        return np.nan, np.nan  # Return NaN if no location is found\n",
    "\n",
    "# Apply the geocoding function to each row and create new columns for latitude and longitude\n",
    "ufo_df[[\"latitude\", \"longitude\"]] = ufo_df.apply(get_coordinates, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Saving the Processed UFO Data üíæ\n",
    "\n",
    "We drop rows without valid coordinates and save the cleaned UFO data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UFO processed data saved to: /home/lferr10/code/leovsferreira/ufo-conspiracy/notebooks/../data/processed/ufo_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Drop rows in the UFO dataset where both 'latitude' and 'longitude' are missing\n",
    "ufo_df.dropna(subset=['latitude', 'longitude'], how='all', inplace=True)\n",
    "\n",
    "# Build the absolute path for the output file\n",
    "ufo_processed_path = os.path.join(PROCESSED_DIR, \"ufo_processed.csv\")\n",
    "\n",
    "# Save the processed UFO data to a CSV file in the processed data folder\n",
    "ufo_df.to_csv(ufo_processed_path, index=False)\n",
    "\n",
    "print(\"UFO processed data saved to:\", ufo_processed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ufo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
